{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b1085ac",
   "metadata": {},
   "source": [
    "# Imports\n",
    "Import required libraries used in this notebook:\n",
    "- `requests` — HTTP requests\n",
    "- `bs4` (`BeautifulSoup`) — HTML parsing\n",
    "- `pandas` — data storage and CSV export\n",
    "- `time` — polite request delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4b55afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35e7f1",
   "metadata": {},
   "source": [
    "# Amazon Multipage Scraper\n",
    "This notebook scrapes Amazon search results across multiple pages and saves product data to `amazon_products.csv`.\n",
    "- **Purpose:** Collect product title, price, rating, and product link from search results.\n",
    "- **Prerequisites:** Install `requests`, `beautifulsoup4`, `pandas` (the notebook kernel already includes these).\n",
    "- **Run order:** Execute cells top-to-bottom. Stop or reduce `NUM_PAGES` if you encounter CAPTCHAs or rate limiting.\n",
    "- **Politeness & legality:** Use a short delay between requests (`time.sleep(1)`) and respect Amazon's terms of service and robots.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "050e017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base Amazon search URL\n",
    "BASE_URL = \"https://www.amazon.in/s?k=playstation+5&crid=302ZJMTZG1JP1&sprefix=playstation+%2Caps%2C369&ref=nb_sb_noss_2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db7ef67",
   "metadata": {},
   "source": [
    "### Multipage Scraper\n",
    "This scraper is designed to iterate search result pages by appending a `&page={n}` parameter to the base search URL.\n",
    "- Set `NUM_PAGES` to control how many pages to scrape.\n",
    "- Each page is fetched, parsed, and product entries are extracted with `extract_product_data(product)`.\n",
    "- The collected rows are stored in `all_data` and written to CSV at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98bb349",
   "metadata": {},
   "source": [
    "### Search URL (BASE_URL)\n",
    "Set the `BASE_URL` to the Amazon search page you want to scrape. Example:\n",
    "- `https://www.amazon.in/s?k=playstation+5`\n",
    "Notes:\n",
    "- Do not include the `&page=` parameter in `BASE_URL`; the pagination loop will append `&page={page}` automatically.\n",
    "- If you need a different locale or query, update `BASE_URL` accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d6898c",
   "metadata": {},
   "source": [
    "### Request Headers\n",
    "Set `HEADERS` to mimic a browser's `User-Agent` and `Accept-Language`.\n",
    "- Passed to `requests.get()` to reduce blocking.\n",
    "- You can update the `User-Agent` string if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "133da887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set request headers to mimic a browser\n",
    "HEADERS = {\n",
    "    'user-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36 Edg/143.0.0.0',\n",
    "    'Accept-Language': 'en-US, en;q=0.5'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08852ab",
   "metadata": {},
   "source": [
    "### Fetch Page Function\n",
    "`fetch_amazon_page(url, headers)` makes a GET request to `url` with `HEADERS` and returns a `BeautifulSoup` object on success (or `None` on failure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d643f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch and parse a single Amazon search result page\n",
    "def fetch_amazon_page(url, headers):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            return BeautifulSoup(response.content, 'html.parser')\n",
    "        else:\n",
    "            print(f\"Failed to fetch {url} (status code: {response.status_code})\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea4b170",
   "metadata": {},
   "source": [
    "### Pagination Settings\n",
    "Set `NUM_PAGES` to control how many search result pages to scrape and initialize `all_data` to collect extracted rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a1aca0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set how many pages to scrape and initialize data list\n",
    "NUM_PAGES = 5  # Change this to scrape more/less pages\n",
    "all_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc37666",
   "metadata": {},
   "source": [
    "### Request Headers\n",
    "To reduce chances of blocking, the scraper sends a `User-Agent` and minimal headers that mimic a browser.\n",
    "- `HEADERS` is a dictionary provided to `requests.get()`.\n",
    "- You can update the `User-Agent` string if needed, but avoid automated rapid changes that appear suspicious.\n",
    "- Consider adding randomized delays, retries, or proxies for larger scraping tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67876b41",
   "metadata": {},
   "source": [
    "### Send HTTP Request\n",
    "- `webpage = requests.get(URL, headers=HEADERS)`: Sends a GET request to the Amazon search URL with the specified headers and stores the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4586e05",
   "metadata": {},
   "source": [
    "### Display Response Object\n",
    "- `webpage`: Displays the response object to check the status of the HTTP request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82318bbb",
   "metadata": {},
   "source": [
    "### View Raw HTML Content\n",
    "- `webpage.content`: Displays the raw HTML content of the fetched web page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccc9b82",
   "metadata": {},
   "source": [
    "### Check Content Type\n",
    "- `type(webpage.content)`: Checks the data type of the web page content (should be bytes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb371c26",
   "metadata": {},
   "source": [
    "### Parse HTML with BeautifulSoup\n",
    "- `soup = BeautifulSoup(webpage.content, 'html.parser')`: Parses the HTML content using BeautifulSoup for further data extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca01fa3d",
   "metadata": {},
   "source": [
    "### Display Parsed HTML Object\n",
    "- `soup`: Displays the BeautifulSoup object to inspect the parsed HTML structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2a1b7b",
   "metadata": {},
   "source": [
    "### Find Product Links\n",
    "- `links = soup.find_all(...)`: Finds all anchor tags with specific classes that likely contain product links from the search results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837dc5d9",
   "metadata": {},
   "source": [
    "### Display Product Links\n",
    "- `links`: Displays the list of found product link elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d29aab",
   "metadata": {},
   "source": [
    "### Extract a Product Link\n",
    "- `link = links[1].get('href')`: Extracts the href attribute from the second product link in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32667765",
   "metadata": {},
   "source": [
    "### Build Full Product URL\n",
    "- `product_list = 'https://amazon.com'+link`: Constructs the full URL for the selected product by combining the base URL with the extracted link."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9db8263",
   "metadata": {},
   "source": [
    "### Display Product URL\n",
    "- `product_list`: Displays the constructed product URL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f3e91",
   "metadata": {},
   "source": [
    "### Fetch Product Page\n",
    "- `new_webpage = requests.get(product_list, headers=HEADERS)`: Sends a GET request to the product page URL to fetch its HTML content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba438871",
   "metadata": {},
   "source": [
    "### Display Product Page Response\n",
    "- `new_webpage`: Displays the response object for the product page request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b12b4bc",
   "metadata": {},
   "source": [
    "### Parse Product Page HTML\n",
    "- `new_soup = BeautifulSoup(new_webpage.content, 'html.parser')`: Parses the product page HTML content for data extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ec042",
   "metadata": {},
   "source": [
    "### Display Parsed Product Page\n",
    "- `new_soup`: Displays the BeautifulSoup object for the product page to inspect its structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88073e90",
   "metadata": {},
   "source": [
    "### Find Product Title Element\n",
    "- `product = new_soup.find('span', attrs={'id':'productTitle'})`: Searches for the product title element by its ID in the parsed product page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a54595d",
   "metadata": {},
   "source": [
    "### Display Product Title Element\n",
    "- `product`: Displays the found product title element (or None if not found)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "146633ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Removed: single-page product title print, not needed for multipage automation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b570096",
   "metadata": {},
   "source": [
    "### Extract and Print Product Title\n",
    "- Checks if the product element was found.\n",
    "- If found, extracts and prints the product title text.\n",
    "- If not found, prints a message indicating the title was not found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7a5247",
   "metadata": {},
   "source": [
    "### Extract Product Data\n",
    "`extract_product_data(product)` extracts `Title`, `Price`, `Rating`, and `Link` from an individual product container and appends a dictionary to `all_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "252d109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract product details from a single product element and append to all_data\n",
    "def extract_product_data(product):\n",
    "    title_elem = product.find('span', {'class': 'a-size-medium a-color-base a-text-normal'})\n",
    "    if not title_elem:\n",
    "        h2_elem = product.find('h2')\n",
    "        if h2_elem:\n",
    "            title_elem = h2_elem.find('span')\n",
    "    title = title_elem.text.strip() if title_elem else None\n",
    "    price_whole = product.find('span', {'class': 'a-price-whole'})\n",
    "    price_fraction = product.find('span', {'class': 'a-price-fraction'})\n",
    "    rating_elem = product.find('span', {'class': 'a-icon-alt'})\n",
    "    link_elem = product.find('a', {'class': 'a-link-normal s-no-outline'})\n",
    "    price = None\n",
    "    if price_whole and price_fraction:\n",
    "        price = price_whole.text.strip() + price_fraction.text.strip()\n",
    "    elif price_whole:\n",
    "        price = price_whole.text.strip()\n",
    "    rating = rating_elem.text.strip() if rating_elem else None\n",
    "    link = 'https://www.amazon.in' + link_elem['href'] if link_elem else None\n",
    "    all_data.append({'Title': title, 'Price': price, 'Rating': rating, 'Link': link})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5138fd58",
   "metadata": {},
   "source": [
    "### Extract Product Title Using Selenium\n",
    "- Imports Selenium and related modules for browser automation.\n",
    "- Sets up Chrome browser in headless mode.\n",
    "- Opens the product page and waits for it to load.\n",
    "- Tries to find and print the product title using Selenium.\n",
    "- Handles exceptions if the title is not found.\n",
    "- Closes the browser after extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504a83dc",
   "metadata": {},
   "source": [
    "### Pagination Loop\n",
    "Main loop: iterate `page` from `1` to `NUM_PAGES`, fetch each paged URL, parse product containers, call `extract_product_data` for each product, and sleep briefly between requests to be polite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6f882480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1...\n",
      "Fetching page 2...\n",
      "Fetching page 3...\n",
      "Fetching page 4...\n",
      "Fetching page 5...\n"
     ]
    }
   ],
   "source": [
    "# Loop through multiple pages and collect product data\n",
    "for page in range(1, NUM_PAGES + 1):\n",
    "    paged_url = BASE_URL + f'&page={page}'\n",
    "    print(f\"Fetching page {page}...\")\n",
    "    soup = fetch_amazon_page(paged_url, HEADERS)\n",
    "    if soup is None:\n",
    "        continue\n",
    "    products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "    for product in products:\n",
    "        extract_product_data(product)\n",
    "    time.sleep(1)  # Be polite to Amazon's servers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d53b904",
   "metadata": {},
   "source": [
    "### Parse and Extract Product Data from Search Results\n",
    "- Imports BeautifulSoup and pandas again (for clarity in this cell).\n",
    "- Parses the original search results page.\n",
    "- Finds all product containers on the page.\n",
    "- For each product, extracts:\n",
    "  - Title\n",
    "  - Price (whole and fraction)\n",
    "  - Rating\n",
    "  - Product link\n",
    "- Appends the extracted data as a dictionary to a list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5e7904",
   "metadata": {},
   "source": [
    "### Save Extracted Data to CSV\n",
    "After scraping completes the collected data in `all_data` is converted to a `pandas.DataFrame` and saved as `amazon_products.csv`.\n",
    "- The CSV includes columns: `title`, `price`, `rating`, `link`.\n",
    "- Post-processing suggestions: normalize `price` to numeric, remove duplicate rows, and validate `link` values before analysis.\n",
    "- To re-run: adjust `NUM_PAGES`, run cells in order, and check `amazon_products.csv` for the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "adad88a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total products scraped: 110\n",
      "Saved all multipage results to amazon_products.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sage Controllers PRO+ Controller compatible wi...</td>\n",
       "      <td>11,700</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>https://www.amazon.in/sspa/click?ie=UTF8&amp;spc=M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sage Controllers PRO+ Controller compatible wi...</td>\n",
       "      <td>14,000</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>https://www.amazon.in/sspa/click?ie=UTF8&amp;spc=M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sony PlayStation5 Gaming Console (Slim)</td>\n",
       "      <td>54,990</td>\n",
       "      <td>4.5 out of 5 stars</td>\n",
       "      <td>https://www.amazon.in/Sony-CFI-2008A01X-PlaySt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sony PlayStation®5 Digital Edition (slim) Cons...</td>\n",
       "      <td>49,990</td>\n",
       "      <td>4.6 out of 5 stars</td>\n",
       "      <td>https://www.amazon.in/Sony-PlayStation%C2%AE5-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sony DualSense Wireless Controller White (Play...</td>\n",
       "      <td>4,890</td>\n",
       "      <td>4.2 out of 5 stars</td>\n",
       "      <td>https://www.amazon.in/DualSense-Wireless-Contr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>OIVO INDIA Dust Protective Cover for PS5 Slim ...</td>\n",
       "      <td>499</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>https://www.amazon.in/OIVO-INDIA-Protective-Ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>PowerA Ultra High Speed HDMI Cable 2.1 For Pla...</td>\n",
       "      <td>1,999</td>\n",
       "      <td>4.6 out of 5 stars</td>\n",
       "      <td>https://www.amazon.in/PowerA-Ultra-HDMI-PlaySt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>SEGA Persona 5 Royal | Standard Edition | Play...</td>\n",
       "      <td>1,968</td>\n",
       "      <td>4.8 out of 5 stars</td>\n",
       "      <td>https://www.amazon.in/Persona-Royal-Standard-P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Sage Controllers PRO+ Controller compatible wi...</td>\n",
       "      <td>11,700</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>https://www.amazon.in/sspa/click?ie=UTF8&amp;spc=M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>GeekShare G.S.TAC Tactical PS5 Controller Skin...</td>\n",
       "      <td>1,520</td>\n",
       "      <td>4.5 out of 5 stars</td>\n",
       "      <td>https://www.amazon.in/sspa/click?ie=UTF8&amp;spc=M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title   Price  \\\n",
       "0    Sage Controllers PRO+ Controller compatible wi...  11,700   \n",
       "1    Sage Controllers PRO+ Controller compatible wi...  14,000   \n",
       "2              Sony PlayStation5 Gaming Console (Slim)  54,990   \n",
       "3    Sony PlayStation®5 Digital Edition (slim) Cons...  49,990   \n",
       "4    Sony DualSense Wireless Controller White (Play...   4,890   \n",
       "..                                                 ...     ...   \n",
       "105  OIVO INDIA Dust Protective Cover for PS5 Slim ...     499   \n",
       "106  PowerA Ultra High Speed HDMI Cable 2.1 For Pla...   1,999   \n",
       "107  SEGA Persona 5 Royal | Standard Edition | Play...   1,968   \n",
       "108  Sage Controllers PRO+ Controller compatible wi...  11,700   \n",
       "109  GeekShare G.S.TAC Tactical PS5 Controller Skin...   1,520   \n",
       "\n",
       "                 Rating                                               Link  \n",
       "0    5.0 out of 5 stars  https://www.amazon.in/sspa/click?ie=UTF8&spc=M...  \n",
       "1    5.0 out of 5 stars  https://www.amazon.in/sspa/click?ie=UTF8&spc=M...  \n",
       "2    4.5 out of 5 stars  https://www.amazon.in/Sony-CFI-2008A01X-PlaySt...  \n",
       "3    4.6 out of 5 stars  https://www.amazon.in/Sony-PlayStation%C2%AE5-...  \n",
       "4    4.2 out of 5 stars  https://www.amazon.in/DualSense-Wireless-Contr...  \n",
       "..                  ...                                                ...  \n",
       "105  5.0 out of 5 stars  https://www.amazon.in/OIVO-INDIA-Protective-Ac...  \n",
       "106  4.6 out of 5 stars  https://www.amazon.in/PowerA-Ultra-HDMI-PlaySt...  \n",
       "107  4.8 out of 5 stars  https://www.amazon.in/Persona-Royal-Standard-P...  \n",
       "108  5.0 out of 5 stars  https://www.amazon.in/sspa/click?ie=UTF8&spc=M...  \n",
       "109  4.5 out of 5 stars  https://www.amazon.in/sspa/click?ie=UTF8&spc=M...  \n",
       "\n",
       "[110 rows x 4 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save all multipage data to CSV and display\n",
    "print(f\"Total products scraped: {len(all_data)}\")\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('amazon_products.csv', index=False)\n",
    "print('Saved all multipage results to amazon_products.csv')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
